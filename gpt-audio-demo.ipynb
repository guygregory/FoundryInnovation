{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f2e71",
   "metadata": {},
   "source": [
    "# GPT-Audio Sample\n",
    "\n",
    "A minimal GPT-Audio demo that generates a spoken announcement from a text prompt using the gpt-audio model. Configure your Azure OpenAI credentials, set the desired prompt, voice, and format, then run the notebook cells to produce an output file and a printed transcript. Inline audio playback is included for quick verification.\n",
    "\n",
    "- Key outputs: `output.wav` (generated audio), transcript printed to the notebook.\n",
    "- Quick steps: set AZURE_OPENAI_API_KEY and AZURE_OPENAI_API_ENDPOINT, adjust `prompt`, `audio_voice`, and `audio_format`, then run cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5aeb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai==1.107.0 dotenv==0.9.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aaaf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load environment variables\n",
    "import base64\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from IPython.display import Audio, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375bde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from environment\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\")\n",
    "model = os.getenv(\"AZURE_OPENAI_AUDIO_MODEL\", \"gpt-audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791481d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional runtime configuration for the audio/chat request\n",
    "modalities = [\"text\", \"audio\"]\n",
    "audio_voice = \"ballad\"  # options: alloy, ash, ballad, cedar, coral, echo, marin, sage, shimmer, verse\n",
    "audio_format = \"wav\"\n",
    "prompt = \"Announce the Grey Matter Tech Summit, hosted at Prospero House in London. Be enthusiastic, and don't include any preamble.\"\n",
    "\n",
    "# Map to the shapes used later in the notebook\n",
    "audio = {\"voice\": audio_voice, \"format\": audio_format}\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "# Basic validation\n",
    "if not api_key or not endpoint:\n",
    "    raise EnvironmentError(\"Environment variables required: AZURE_OPENAI_API_KEY and AZURE_OPENAI_API_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_version=api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d78329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the audio chat completions request\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    modalities=modalities,\n",
    "    audio=audio,\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output audio data to a file\n",
    "wav_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
    "out_path = \"output.wav\"\n",
    "with open(out_path, \"wb\") as f:\n",
    "    f.write(wav_bytes)\n",
    "    # Ensure the bytes are fully flushed to disk on Windows before reading\n",
    "    f.flush()\n",
    "    os.fsync(f.fileno())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-notebook playback via HTML5 audio\n",
    "Audio(filename=out_path,autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the transcript\n",
    "print(completion.choices[0].message.audio.transcript)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
